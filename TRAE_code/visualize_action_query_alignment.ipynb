{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Action Query Alignment\n",
    "\n",
    "This notebook extracts features from VLA and performs grounding.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook loads:\n",
    "1. VLA model (from checkpoint)\n",
    "2. ActionQueryAlignmentHead (from checkpoint)\n",
    "3. GDINO Teacher (for comparison)\n",
    "\n",
    "Then performs grounding on input images using both student and teacher features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "\n",
    "from align_train.models.action_query_alignment_head import ActionQueryAlignmentHead\n",
    "from align_train.models.gdino_teacher import GDINOTeacher\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticProcessor\n",
    "from prismatic.vla.constants import NUM_TOKENS\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the configuration for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Configuration ============\n",
    "vla_checkpoint_path = \"outputs/VLA-Adapter--libero_spatial_no_noops--20250210_120000/\"\n",
    "action_query_alignment_head_path = \"outputs/VLA-Adapter--libero_spatial_no_noops--20250210_120000/action_query_alignment_head--120000_checkpoint.pt\"\n",
    "\n",
    "# Image path (replace with your image)\n",
    "image_path = \"data/libero/libero_spatial_no_noops/1.0.0/example_image.png\"  # Replace with actual image\n",
    "\n",
    "# Task description\n",
    "task_description = \"pick up the red block\"\n",
    "\n",
    "# GDINO Teacher paths\n",
    "gdino_config_path = \"/tmp/Distill-VLA/visual_teacher/Open-GroundingDino/config/cfg_odvg.py\"\n",
    "gdino_checkpoint_path = \"/tmp/Distill-VLA/checkpoints/open_gdino_finetuned/checkpoint_best_regular.pth\"\n",
    "\n",
    "# Device\n",
    "device = \"cuda:0\"\n",
    "\n",
    "print(\"Configuration set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "Load the VLA model, ActionQueryAlignmentHead, and GDINO Teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading Models\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============ Load VLA Model ============\n",
    "print(f\"\\n[1/3] Loading VLA model from: {vla_checkpoint_path}\")\n",
    "vla = OpenVLAForActionPrediction.from_pretrained(vla_checkpoint_path)\n",
    "vla = vla.to(device).eval()\n",
    "llm_dim = vla.llm_dim\n",
    "print(f\"  ? VLA loaded successfully (llm_dim={llm_dim})\")\n",
    "\n",
    "# ============ Load ActionQueryAlignmentHead ============\n",
    "action_query_alignment_head = None\n",
    "if action_query_alignment_head_path and os.path.exists(action_query_alignment_head_path):\n",
    "    print(f\"\\n[2/3] Loading ActionQueryAlignmentHead from: {action_query_alignment_head_path}\")\n",
    "    action_query_alignment_head = ActionQueryAlignmentHead(\n",
    "        input_dim=llm_dim,\n",
    "        gdino_dim=256,\n",
    "        dropout=0.1,\n",
    "    ).to(torch.float32).to(device).eval()\n",
    "    \n",
    "    state_dict = torch.load(action_query_alignment_head_path, weights_only=True, map_location=device)
    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
    action_query_alignment_head.load_state_dict(new_state_dict)\n",
    "    print(f\"  ? ActionQueryAlignmentHead loaded successfully\")\n",
    "else:\n",
    "    print(f\"\\n[2/3] ActionQueryAlignmentHead not found or not specified, skipping...\")\n",
    "\n",
    "# ============ Load GDINO Teacher ============\n",
    "print(f\"\\n[3/3] Loading GDINO Teacher...\")\n",
    "gdino_teacher = GDINOTeacher(\n",
    "    config_path=gdino_config_path,\n",
    "    checkpoint_path=gdino_checkpoint_path,\n",
    "    device=device,\n",
    ")\n",
    "print(f\"  ? GDINO Teacher loaded successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processor\n",
    "\n",
    "Load the PrismaticProcessor for processing inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = PrismaticProcessor.from_pretrained(vla_checkpoint_path)\n",
    "print(\"Processor loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image\n",
    "\n",
    "Load the input image for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading image from: {image_path}\")\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "print(f\"  ? Image loaded: {image.size}\")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "Define functions for extracting action queries and performing grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action_queries(\n",
    "    vla: OpenVLAForActionPrediction,\n",
    "    processor: PrismaticProcessor,\n",
    "    image: Image.Image,\n",
    "    task_description: str = \"pick up the red block\",\n",
    "    device: str = \"cuda:0\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract action queries from VLA model.\n",
    "    \n",
    "    Args:\n",
    "        vla: VLA model\n",
    "        processor: PrismaticProcessor\n",
    "        image: Input image (PIL Image)\n",
    "        task_description: Task description text\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        action_queries: Action query hidden states [B, 64, 896]\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=task_description,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = vla(**inputs)\n",
    "    \n",
    "    # Extract action queries from LLM output\n",
    "    # VLA output structure: hidden_states[-1] has shape [B, seq_len, 896]\n",
    "    last_hidden = outputs.hidden_states[-1]  # [B, seq_len, 896]\n",
    "    \n",
    "    # Sequence structure: [CLS] + [vision_patches] + [text_tokens] + [action_tokens]\n",
    "    # We need to extract action tokens (last 64 tokens)\n",
    "    action_queries = last_hidden[:, -NUM_TOKENS:, :]  # [B, 64, 896]\n",
    "    \n",
    "    return action_queries\n",
    "\n",
    "\n",
    "def perform_grounding_student(\n",
    "    action_queries: torch.Tensor,\n",
    "    action_query_alignment_head: ActionQueryAlignmentHead,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform grounding using student features (via ActionQueryAlignmentHead).\n",
    "    \n",
    "    Args:\n",
    "        action_queries: Action query hidden states [B, 64, 896]\n",
    "        action_query_alignment_head: ActionQueryAlignmentHead\n",
    "        \n",
    "    Returns:\n",
    "        student_hs: Student hidden states [B, 900, 256]\n",
    "        student_ref: Student reference points (bboxes) [B, 900, 4]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        student_hs, student_ref = action_query_alignment_head(action_queries)\n",
    "    \n",
    "    return student_hs, student_ref\n",
    "\n",
    "\n",
    "def perform_grounding_teacher(\n",
    "    gdino_teacher: GDINOTeacher,\n",
    "    image: Image.Image,\n",
    "    task_description: str = \"pick up the red block\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform grounding using teacher (GDINO).\n",
    "    \n",
    "    Args:\n",
    "        gdino_teacher: GDINO Teacher model\n",
    "        image: Input image (PIL Image)\n",
    "        task_description: Task description text\n",
    "        \n",
    "    Returns:\n",
    "        teacher_hs: Teacher hidden states [B, 900, 256]\n",
    "        teacher_ref: Teacher reference points (bboxes) [B, 900, 4]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = gdino_teacher.forward_with_intermediates(\n",
    "            images=image,\n",
    "            captions=[task_description],\n",
    "        )\n",
    "    \n",
    "    teacher_hs = outputs[\"hs\"]\n",
    "    teacher_ref = outputs[\"ref\"]\n",
    "    \n",
    "    return teacher_hs, teacher_ref\n",
    "\n",
    "\n",
    "def compute_feature_similarity(\n",
    "    student_hs: torch.Tensor,\n",
    "    teacher_hs: torch.Tensor,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute similarity metrics between student and teacher features.\n",
    "    \n",
    "    Args:\n",
    "        student_hs: Student hidden states [B, 900, 256]\n",
    "        teacher_hs: Teacher hidden states [B, 900, 256]\n",
    "        \n",
    "    Returns:\n",
    "        Dict with similarity metrics\n",
    "    \"\"\"\n",
    "    # Flatten features\n",
    "    student_flat = student_hs.flatten()  # [B*900*256]\n",
    "    teacher_flat = teacher_hs.flatten()  # [B*900*256]\n",
    "    \n",
    "    # Cosine similarity\n",
    "    student_norm = F.normalize(student_flat, p=2, dim=0)\n",
    "    teacher_norm = F.normalize(teacher_flat, p=2, dim=0)\n",
    "    cosine_sim = torch.dot(student_norm, teacher_norm).item()\n",
    "    \n",
    "    # MSE\n",
    "    mse = F.mse_loss(student_hs, teacher_hs).item()\n",
    "    \n",
    "    # L1\n",
    "    l1 = F.l1_loss(student_hs, teacher_hs).item()\n",
    "    \n",
    "    return {\n",
    "        \"cosine_similarity\": cosine_sim,\n",
    "        \"mse\": mse,\n",
    "        \"l1\": l1,\n",
    "    }\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Action Queries from VLA\n",
    "\n",
    "Extract action queries from the VLA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nExtracting action queries from VLA...\")\n",
    "action_queries = extract_action_queries(\n",
    "    vla=vla,\n",
    "    processor=processor,\n",
    "    image=image,\n",
    "    task_description=task_description,\n",
    "    device=device,\n",
    ")\n",
    "print(f\"  ? Action queries extracted: {action_queries.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grounding with Student\n",
    "\n",
    "Perform grounding using the student model (ActionQueryAlignmentHead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action_query_alignment_head is not None:\n",
    "    print(f\"\\nPerforming grounding with student (ActionQueryAlignmentHead)...\")\n",
    "    student_hs, student_ref = perform_grounding_student(\n",
    "        action_queries=action_queries,\n",
    "        action_query_alignment_head=action_query_alignment_head,\n",
    "    )\n",
    "    print(f\"  ? Student features: hs={student_hs.shape}, ref={student_ref.shape}\")\n",
    "else:\n",
    "    print(\"\\nSkipping student grounding (ActionQueryAlignmentHead not loaded)\")\n",
    "    student_hs, student_ref = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grounding with Teacher\n",
    "\n",
    "Perform grounding using the teacher model (GDINO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPerforming grounding with teacher (GDINO)...\")\n",
    "teacher_hs, teacher_ref = perform_grounding_teacher(\n",
    "    gdino_teacher=gdino_teacher,\n",
    "    image=image,\n",
    "    task_description=task_description,\n",
    ")\n",
    "print(f\"  ? Teacher features: hs={teacher_hs.shape}, ref={teacher_ref.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Feature Similarity\n",
    "\n",
    "Compute similarity metrics between student and teacher features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if student_hs is not None:\n",
    "    print(f\"\\nComputing feature similarity...\")\n",
    "    similarity_metrics = compute_feature_similarity(\n",
    "        student_hs=student_hs,\n",
    "        teacher_hs=teacher_hs,\n",
    "    )\n",
    "    print(f\"  ? Similarity metrics:\")\n",
    "    for key, value in similarity_metrics.items():\n",
    "        print(f\"      {key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"\\nSkipping feature similarity computation (student features not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Grounding Results\n",
    "\n",
    "Visualize the grounding results from both student and teacher models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grounding_results(\n",
    "    image: Image.Image,\n",
    "    student_ref: torch.Tensor,\n",
    "    teacher_ref: torch.Tensor,\n",
    "    task_description: str,\n",
    "    save_path: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize grounding results from student and teacher.\n",
    "    \n",
    "    Args:\n",
    "        image: Original image\n",
    "        student_ref: Student reference points [B, 900, 4]\n",
    "        teacher_ref: Teacher reference points [B, 900, 4]\n",
    "        task_description: Task description\n",
    "        save_path: Path to save visualization (optional)\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    image_np = np.array(image)\n",
    "    height, width = image_np.shape[:2]\n",
    "    \n",
    "    # Convert reference points to bboxes [x, y, w, h] in pixel coordinates\n",
    "    student_ref_np = student_ref[0].cpu().numpy()  # [900, 4]\n",
    "    teacher_ref_np = teacher_ref[0].cpu().numpy()  # [900, 4]\n",
    "    \n",
    "    # Convert from normalized [cx, cy, w, h] to pixel coordinates\n",
    "    student_bboxes = student_ref_np.copy()\n",
    "    student_bboxes[:, 0] *= width  # cx\n",
    "    student_bboxes[:, 1] *= height  # cy\n",
    "    student_bboxes[:, 2] *= width  # w\n",
    "    student_bboxes[:, 3] *= height  # h\n",
    "    \n",
    "    teacher_bboxes = teacher_ref_np.copy()\n",
    "    teacher_bboxes[:, 0] *= width  # cx\n",
    "    teacher_bboxes[:, 1] *= height  # cy\n",
    "    teacher_bboxes[:, 2] *= width  # w\n",
    "    teacher_bboxes[:, 3] *= height  # h\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Original image\n",
    "    axes[0].imshow(image_np)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot 2: Student bboxes (from ActionQueryAlignmentHead)\n",
    "    image_copy1 = image_np.copy()\n",
    "    axes[1].imshow(image_copy1)\n",
    "    axes[1].set_title(f\"Student (ActionQueryAlignmentHead)\\n{task_description}\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw top-20 student bboxes with highest confidence\n",
    "    for i in range(min(20, len(student_bboxes))):\n",
    "        cx, cy, w, h = student_bboxes[i]\n",
    "        x1, y1 = int(cx - w/2), int(cy - h/2)\n",
    "        x2, y2 = int(cx + w/2), int(cy + h/2)\n",
    "        cv2.rectangle(image_copy1, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    axes[1].imshow(image_copy1)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Plot 3: Teacher bboxes (from GDINO)\n",
    "    image_copy2 = image_np.copy()\n",
    "    axes[2].imshow(image_copy2)\n",
    "    axes[2].set_title(f\"Teacher (GDINO)\\n{task_description}\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw top-20 teacher bboxes with highest confidence\n",
    "    for i in range(min(20, len(teacher_bboxes))):\n",
    "        cx, cy, w, h = teacher_bboxes[i]\n",
    "        x1, y1 = int(cx - w/2), int(cy - h/2)\n",
    "        x2, y2 = int(cx + w/2), int(cy + h/2)\n",
    "        cv2.rectangle(image_copy2, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "    \n",
    "    axes[2].imshow(image_copy2)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  ? Visualization saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Visualization\n",
    "\n",
    "Run the visualization and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if student_ref is not None:\n",
    "    print(f\"\\nVisualizing grounding results...\")\n",
    "    visualize_grounding_results(\n",
    "        image=image,\n",
    "        student_ref=student_ref,\n",
    "        teacher_ref=teacher_ref,\n",
    "        task_description=task_description,\n",
    "        save_path=\"action_query_alignment_visualization.png\",\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nSkipping visualization (student reference points not available)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Visualization complete!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
